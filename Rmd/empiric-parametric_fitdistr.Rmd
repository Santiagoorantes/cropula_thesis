---
title: "Empirical and Parametric Distribution Fitting"
author: "Santiago Orantes"
date: "30/1/2022"
output:
  rmdformats::readthedown:
    code_folding: show
    self_contained: true
    thumbnails: false
    lightbox: false
pkgdown:
  as_is: true
---

\DeclareMathOperator*{\E}{\mathbb{E}}
\DeclareMathOperator*{\V}{\mathbb{V}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Kernels

## Standard kernel density estimator

The input of a kernel density estimator is a vector $y_1, \cdots, y_T$ of independent realizations from the unknown density $f_Y$.

The **Standard Kernel** density estimator places a *bump* or *individual kernel* at each realization and the estimate of the density at any point is the **sum** of the **individual kernels** at that point.

```{r, fig.align='center', fig.width=8}
### Standard kernel example

# generic yield samples
yields <- c(87, 120, 130, 135, 140, 145, 158)
mu <- mean(yields)
#sig <- sd(yields)
sig <- 12

par(mfrow = c(1,2))
# Histogram and kernel with density function
hist(rnorm(10000, mean = mu, sd = sig), probability = TRUE, breaks = 100)
plot(density(yields))
```


```{r, fig.align='center', fig.width=8}
# Creating simple kernel based on normal distributions
x <- seq(0,200,0.01)
ker <- 0
for (i in seq_along(yields)) {
  probs <- dnorm(x, mean = yields[i], sd = sig)
  ker <- ker + probs
}
## Plotting the Kernel and the individual normals
plot(x = x, y = ker, type = "l", lwd = 2.5, lty = 2, col = "darkblue",
     xlab = "Yield", ylab = "Kernel Density",
     main = "Standard Kernel (normal margins)")
for (i in seq_along(yields)) {
  probs <- dnorm(x, mean = yields[i], sd = sig)
  lines(x = x, y = probs, col = "darkgreen")
}
points(x = yields, y = rep(0, length(yields)), pch = 19, col = "darkgreen")

```

The non-parametric kernel estimate of $f_Y$ at any point (say $y_0$) is defined as:
$$\hat{f}_Y(y_0) = \sum_{i=1}^{T}\frac{K\left(\frac{y_0-y_i}{h}\right)}{Th}$$
where,

* K(.): Kernel function. 
* h: smoothing parameter.

So we need to decide 2 things: **Kernel function** and **smoothing parameter**.

**SilvermanÂ´s rule of thumb**: 
$\hat{h}=0.9*\text{min}\left[\text{standard dev}, \frac{\text{interquartile range}}{1.34} \right]*T^{-(1/5)}$

## Adaptive kernel density estimator

The adaptive kernel allows the smoothing parameter to vary, so we have a vector of smoothing parameters of the same length as the data.

### Example

In the particular case of crops, we are concerned with under-smoothing in the tails, we desire our smoothing parameters to be inversely related to the denseness of the data.

If the true density were known, we could compare the realization to the true density and make a decision regarding the necessary smoothness for its individual kernel. If we do not know the true density, a pilot estimate of the density needs to be used; For example: the standard kernel estimate.

Let $\check{f}$ denote the standard kernel (pilot), the local scale $\lambda_i$ for $\alpha\in[0,1]$ is defined as:  

$$\lambda_i = \left( \frac{\check{f}(y_i)}{g} \right)^{-\alpha}$$

where,

$$
\begin{aligned}
\log(g) &= \frac{1}{T}\sum\log\check{f}(y_i) \implies g = \exp\left(\frac{1}{T}\sum\log\check{f}(y_i)\right) \\
 & \implies g = \left(\check{f}(y_1)\cdot \ldots \cdot \check{f}(y_T)\right)^{1/T} \\
 & \implies \lambda_i = \left( \frac{\check{f}(y_i)}{\left(\check{f}(y_1)\ldots \check{f}(y_T)\right)^{1/T}} \right)^{-\alpha}
\end{aligned}
$$ 

So that the **adaptive kernel** estimate of $f_Y$ at any given point is defined as:

$$\hat{f}_Y(y_0) = \sum_{i=1}^{T}\frac{K\left(\frac{y_0-y_i}{h\cdot\lambda_i}\right)}{T\cdot h\cdot\lambda_i}$$

If $\alpha = 0$ we have the *standard kernel*; we will choose for example $alpha = 1/2$.  

```{r}
# Observations
yields <- c(87, 120, 130, 135, 140, 145, 158)


# fixed parameters
alpha <- 1/2
T <- length(yields)
# Silverman's rule of thumb
h <- 0.9 * min(sd(yields), IQR(yields)/1.34) * T^(-1/5)

```


# Parametric distributions

## Beta distribution

The beta distribution $X\sim Beta(\alpha, \beta)$ as pdf:

$$
\begin{aligned}
f_X(x) &= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}, \quad x\in[0,1]
\end{aligned}
$$

And the **mean** and **variance** are given by

$$
\begin{aligned}
\E[X]= \frac{\alpha}{\alpha+\beta}, \quad \V[X] = \frac{\alpha\beta}{(\alpha+\beta+1)(\alpha+\beta)^2}
\end{aligned}
$$

Now, if we want to fit a **beta distribution** to the yield of a certain crop, or more generally to any data that we have which is not between $(0,1)$, we can *shift* the distribution to make it take values between $(a,b)$. Then we get again a **beta** distribution with 4 parameters ($\alpha, \beta, a, b$).

Let $X\sim Beta(\alpha, \beta)$ and $Y = a + (b-a)X$, $\implies Y \sim Beta(\alpha, \beta, a, b)$.

Where $Y$ has the pdf:

$$
\begin{aligned}
f_Y(y) &= \frac{1}{b-a}f_X\left(\frac{y-a}{b-a}\right)=\frac{1}{b-a}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\left(\frac{y-a}{b-a}\right)^{\alpha-1}\left(1-\left(\frac{y-a}{b-a}\right)\right)^{\beta-1}, \quad x\in[a,b]
\end{aligned}
$$


> **Proof (using Theorem of change of variable):**  
> By the theorem of change of variable we have $f_Y(\psi^{{-1}}(y))\left|\frac{d}{dy}\psi^{{-1}}(y)\right|$  
> $Y=a+(b-a)X \implies x = (y-a)/(b-a)$ 
> $$
> \begin{aligned}
> \psi^{-1}(y)=x=\frac{y-a}{b-a} \implies f_y(y)&=f_X\left(\frac{y-a}{b-a}\right)\left|\frac{d}{dy}\frac{y-a}{b-a}\right| \\
> &= f_X\left(\frac{y-a}{b-a}\right)\frac{1}{b-a}
> \end{aligned}
> $$

With **mean** and **variance** given by

$$
\begin{aligned}
\E[X]= a+(b-a)\cdot\frac{\alpha}{\alpha+\beta}, \quad \V[X] = (b-a)^2\cdot\frac{\alpha\beta}{(\alpha+\beta+1)(\alpha+\beta)^2}
\end{aligned}
$$

In the particular case of the distribution of the **Yield** of crops, we know that the yield is cannot be negative and therefore $a=0$, leaving us with $Y\sim Beta(\alpha, \beta, b)$:

$$
\begin{aligned}
f_Y(y) = \frac{1}{b}f_X\left(\frac{y}{b}\right) &=\frac{1}{b}\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\left(\frac{y}{b}\right)^{\alpha-1}\left(1-\frac{y}{b}\right)^{\beta-1} \\
&= \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\cdot\frac{y^{\alpha-1}(b-y)^{\beta-1}}{b^{\alpha+\beta-1}}
\end{aligned}
$$

With **mean** and **variance** given by

$$
\begin{aligned}
\E[X]= b\cdot\frac{\alpha}{\alpha+\beta}, \quad \V[X] = b^2\cdot\frac{\alpha\beta}{(\alpha+\beta+1)(\alpha+\beta)^2}
\end{aligned}
$$

Therefore we need to set beforehand the **maximum** attainable value for the **yield** that we are modelling, which is given by the parameter **$b$**.

Of course, theoretically speaking, there is no actual upper bound for the yield of a crop; but it can also be reasonable to assume that it is actually bounded since crops cannot produce infinite yields. We can assume that the maximum yield would be attained under ideal conditions where the crop survives fully and produces the maximum physically attainable yield. So by setting $b$ as some number right to the maximum historical yield we are therefore saying that the probability of having a higher yield than $b$ is zero.

```{r, message=FALSE}
library(fitdistrplus, quietly = TRUE)
library(ExtDist, quietly = TRUE)
# Assume the following historical yields in Tons/Hect
y <- c(13.153264,12.354199,10.670869,13.246366,13.205569,12.983425,12.484264,12.668871,4.305155,12.400265,9.585478, 12.091149,12.089205,11.569874,12.059283,12.526045,13.045674,12.522453)

# Setting maximum attainable yield to maximum historical + 1 standard deviation (empirical)
b <- round(max(y) + sd(y))
a <- 0

y_fix <- y / b

fit_genbeta <- fitdist(y_fix, distr = "beta", method = c("mle"))
summary(fit_genbeta)
plot(fit_genbeta)

(f1 <- fitdist(y_fix, "beta", method="mge", gof="CvM"))
(f2 <- fitdist(y_fix, "beta", method="mge", gof="KS"))
(f3 <- fitdist(y_fix, "beta", method="mge", gof="AD"))
(f4 <- fitdist(y_fix, "beta", method="mge", gof="ADR"))
(f5 <- fitdist(y_fix, "beta", method="mge", gof="ADL"))
(f6 <- fitdist(y_fix, "beta", method="mge", gof="AD2R"))

cdfcomp(list(f1, f2, f3, f4, f5, f6))
denscomp(list(f1, f2, f3, f4, f5, f6))

```


